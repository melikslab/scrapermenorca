name: Scraper diario + Pages

on:
  schedule:
    - cron: "0 4 * * *"   # 04:00 UTC ≈ 06:00 Madrid
  workflow_dispatch:

permissions:
  contents: write         # <- necesario para auto-commit
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Instala deps UNA vez (incluye lxml)
      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip
          pip install requests beautifulsoup4 lxml certifi

      # Asegura carpeta public para que los siguientes pasos no fallen
      - name: Preparar carpeta public
        run: mkdir -p public

      - name: Ejecutar scraper (salida a /public)
        run: python scraper_historico.py
        env:
          DATA_DIR: public   # si tu script ya soporta DATA_DIR; si no, lo quitamos

      - name: Commit JSONs incrementales
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "Datos diarios: propiedades + registro"
          file_pattern: |
            public/propiedades_menorca_estandarizado.json
            public/registro_scraping.json
            public/propiedades_temp.json

      - name: Añadir .nojekyll e índice simple (opcional)
        run: |
          touch public/.nojekyll
          [ -f public/index.html ] || printf '<!doctype html><meta charset="utf-8"><title>Feed</title><h1>Feed JSON</h1><ul><li><a href="propiedades_menorca_estandarizado.json">propiedades</a></li><li><a href="registro_scraping.json">registro</a></li></ul>' > public/index.html

      - name: Configure Pages
        uses: actions/configure-pages@v5

      - name: Upload artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: public

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
